# -*- coding: utf-8 -*-
# Utility functions for data collection
from googlesearch import search
from bs4 import BeautifulSoup
import time
import requests
import os

def get_links(searchStrings, topN):
    ''' send queries to google websearch and return first N results as links
    Parameters:
        searchStrings: list
            list of strings to send as query to google search
        topN: int
            number of results to return
    Returns:
        dict with list of links of length topN for each search query
    '''
    res = dict()

    for l in searchStrings:
        llist = []
        searchGenerator = search(l, stop=topN)
        for i in range(topN):
            try:
                llist.append(searchGenerator.send(None))
            except:
                time.sleep(.5)
        res[l] = llist
    
    return res

def merge_linklists(new, old):
    ''' union new and old linklists
    Parameters:
        new: dict
            new links
        old: dict
            old links
    Returns:
        dict with old and new links
    '''
    for k,v in new.items():
        try: 
            old[k] = list(set(old[k]+v))
        except KeyError:
            old[k] = v

    return(old)


def get_webpage(url):
    ''' send GET request to url and return the content of the response
    Parameters:
        url: string
    Returns:
        webpage-content as bytes object
    '''
    
    headers = {"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        return(res.content)
    except Exception as e:
        print(e)


def save_html(doc, path, fileName):
    ''' parse byte object and save html file to disk
    Paramteters:
        doc: bytes object generated by get_webpace()
        path: string
            folder to save the html file in
    Returns:
        path to where the file is stored
    '''
    try: 
        soup = BeautifulSoup(doc, "html.parser")
        html = soup.prettify("utf-8")
        fPath = os.path.join(path, fileName)
        with open(fPath, "wb") as f:
            f.write(html)
        return fPath
        
    except TypeError:
        print("Empty HTML Response for :", path, "/", fileName)

    

    

def read_html(path):
    ''' load html file from disk
    Parameters:
        path: string
            path to file
    Returns:
        binary object that can be parsed by BeautifulSoup
    '''
    with open(path) as f:
        doc = read(f)

    return doc




